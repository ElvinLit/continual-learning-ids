{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c094fa-8da6-48b6-b19f-1627adfbfcba",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72e802f5-4d04-43b3-ba66-19400b29a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from pyDeepInsight import ImageTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765e6a2d-453f-4b10-8aa3-c4e8f516e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a1d38bf-fc37-42ff-a9a8-c5e35b2978b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "\n",
    "X_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "y = np.where(y == 11, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d0ff6-198a-4f43-bf2c-9c795817ad9e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fefa4ff-04b2-4dda-8b66-08c618fb7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi(normal_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for normal data.\n",
    "    Uses k-Means clustering to partition normal data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        c (int): Number of desired normal concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of normal clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(normal_data)\n",
    "    \n",
    "    normal_concepts = [normal_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating normal concepts\")\n",
    "    \n",
    "    return normal_concepts\n",
    "\n",
    "\n",
    "def create_gamma(anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for anomaly data.\n",
    "    Uses k-Means clustering to partition anomaly data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired anomaly concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of anomaly clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(anomaly_data)\n",
    "    \n",
    "    anomaly_concepts = [anomaly_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating anomaly concepts\")\n",
    "    \n",
    "    return anomaly_concepts\n",
    "    \n",
    "def match_lambda(anomaly_concepts, normal_concepts):\n",
    "    \"\"\"\n",
    "    Matches each normal concept with the closest anomaly concept.\n",
    "    Uses Euclidean distance to determine the best match.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_concepts (list of numpy arrays): List of anomaly clusters.\n",
    "        normal_concepts (list of numpy arrays): List of normal clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Pairs of (normal_concept, matched_anomaly_concept)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    remaining_anomalies = anomaly_concepts.copy()\n",
    "\n",
    "    for normal_concept in normal_concepts:\n",
    "        normal_centroid = np.mean(normal_concept, axis=0)\n",
    "        anomaly_centroids = [np.mean(ac, axis=0) for ac in remaining_anomalies]\n",
    "\n",
    "        distances = cdist([normal_centroid], anomaly_centroids, metric='euclidean')[0]\n",
    "        closest_idx = np.argmin(distances)\n",
    "\n",
    "        pairs.append((normal_concept, remaining_anomalies[closest_idx]))\n",
    "        remaining_anomalies.pop(closest_idx)\n",
    "\n",
    "    print(\"Finished matching concept pairs\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def lifelong_roc_auc(R):\n",
    "    N = R.shape[0]\n",
    "    lower_triangular_sum = np.sum(np.tril(R))\n",
    "    normalization_factor = (N * (N + 1)) / 2\n",
    "\n",
    "    return lower_triangular_sum / normalization_factor\n",
    "\n",
    "def BWT(R):\n",
    "    N = R.shape[0]\n",
    "    backward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            backward_transfer += (R[i, j] - R[j, j])\n",
    "            count += 1\n",
    "\n",
    "    return backward_transfer / count if count > 0 else 0\n",
    "\n",
    "def FWT(R):\n",
    "    N = R.shape[0]\n",
    "    forward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): \n",
    "            forward_transfer += R[i, j]\n",
    "            count += 1\n",
    "\n",
    "    return forward_transfer / count if count > 0 else 0 \n",
    "\n",
    "def kolmogorov_smirnov_test(X_old, X_new, alpha=0.05):\n",
    "    p_values = [ks_2samp(X_old[:, i], X_new[:, i]).pvalue for i in range(X_old.shape[1])]\n",
    "    return np.any(np.array(p_values) < alpha)\n",
    "\n",
    "def histogram_binning(X, bins=25):    \n",
    "    return np.array([np.histogram(X[:, i], bins=bins, density=True)[0] for i in range(X.shape[1])]).T\n",
    "\n",
    "def kl_divergence(P, Q):\n",
    "    P, Q = np.clip(P, 1e-10, None), np.clip(Q, 1e-10, None)  # Avoid log(0)\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "\n",
    "def strategic_sample_selection(X_old, X_new, top_k=100, learning_rate=0.01, num_iterations=100):    \n",
    "    H_old, H_new = histogram_binning(X_old), histogram_binning(X_new)\n",
    "    m_n = np.random.rand(H_new.shape[0])  \n",
    "\n",
    "    def loss_function(m_n):\n",
    "        weighted_H_new = H_new * m_n[:, np.newaxis]  \n",
    "        combined_H = (H_old + weighted_H_new) / 2 \n",
    "        return kl_divergence(H_new, combined_H) \n",
    "\n",
    "    progress_bar = tqdm(total=num_iterations, desc=\"Optimizing Sample Selection\", position=0, leave=True)\n",
    "\n",
    "    def callback(xk):\n",
    "        progress_bar.update(1)  \n",
    "\n",
    "    result = minimize(loss_function, m_n, method=\"L-BFGS-B\", bounds=[(0, 1)] * len(m_n), \n",
    "                      options={\"maxiter\": num_iterations, \"ftol\": 1e-10}, callback=callback)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    selected_indices = np.argsort(result.x)[-top_k:]\n",
    "\n",
    "    return X_new[selected_indices] \n",
    "\n",
    "\n",
    "def update_memory_buffer(X_old, X_new_selected, memory_size=3000):\n",
    "    updated_buffer = np.vstack((X_old, X_new_selected))  \n",
    "\n",
    "    if updated_buffer.shape[0] > memory_size:\n",
    "        updated_buffer = updated_buffer[-memory_size:]\n",
    "\n",
    "    return updated_buffer\n",
    "\n",
    "class AEClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AEClassifier, self).__init__()\n",
    "        nearest_power_of_2 = 2 ** round(np.log2(input_dim))\n",
    "        second_layer = nearest_power_of_2 // 2\n",
    "        third_layer = nearest_power_of_2 // 4\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, second_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(second_layer, third_layer)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(third_layer, second_layer),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(second_layer, input_dim)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        y = self.classifier(x_hat)\n",
    "        return z, x_hat, y\n",
    "\n",
    "\n",
    "class AEWrapper:\n",
    "    def __init__(self, input_dim, device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = AEClassifier(input_dim).to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 20\n",
    "\n",
    "    def fit(self, X):\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        y_tensor = torch.zeros(X_scaled.shape[0]).float().to(self.device)\n",
    "\n",
    "        loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=self.batch_size, shuffle=True)\n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for x_batch, y_batch in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                _, _, out = self.model(x_batch)\n",
    "                loss = self.criterion(out.squeeze(), y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, _, out = self.model(X_tensor)\n",
    "        return -out.squeeze().cpu().numpy()\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(32, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(32, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # For scaled input [0, 1]\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, logvar\n",
    "\n",
    "def elbo_loss(x, x_hat, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(x_hat, x, reduction='mean')\n",
    "    kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_div\n",
    "\n",
    "class HierarchicalMemory:\n",
    "    def __init__(self, W_epsilon=1.5):\n",
    "        self.memory = {}  # (level, idx) -> {data, parent}\n",
    "        self.level_index_counter = {}\n",
    "        self.W_epsilon = W_epsilon\n",
    "\n",
    "    def _next_index(self, level):\n",
    "        if level not in self.level_index_counter:\n",
    "            self.level_index_counter[level] = 0\n",
    "        idx = self.level_index_counter[level]\n",
    "        self.level_index_counter[level] += 1\n",
    "        return idx\n",
    "\n",
    "    def add_root_concept(self, data):\n",
    "        key = (1, self._next_index(1))\n",
    "        self.memory[key] = {\"data\": data, \"parent\": None}\n",
    "        return key\n",
    "\n",
    "    def add_sub_concept(self, parent_key, data):\n",
    "        level = parent_key[0] + 1\n",
    "        key = (level, self._next_index(level))\n",
    "        self.memory[key] = {\"data\": data, \"parent\": parent_key}\n",
    "        return key\n",
    "\n",
    "    def consolidate(self, new_data):\n",
    "        if not self.memory:\n",
    "            return self.add_root_concept(new_data)\n",
    "\n",
    "        best_key, best_distance = None, float(\"inf\")\n",
    "        new_mean = np.mean(new_data, axis=0)\n",
    "\n",
    "        for key, val in self.memory.items():\n",
    "            mean_existing = np.mean(val[\"data\"], axis=0)\n",
    "            dist = wasserstein_distance(mean_existing, new_mean)\n",
    "            if dist < best_distance:\n",
    "                best_distance = dist\n",
    "                best_key = key\n",
    "\n",
    "        if best_distance < self.W_epsilon:\n",
    "            return self.add_sub_concept(best_key, new_data)\n",
    "        else:\n",
    "            return self.add_root_concept(new_data)\n",
    "\n",
    "    def get_all_data(self):\n",
    "        return np.vstack([v[\"data\"] for v in self.memory.values()])\n",
    "\n",
    "class VLADWrapper:\n",
    "    def __init__(self, input_dim, device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = VAE(input_dim=input_dim).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.batch_size = 128\n",
    "        self.epochs = 20\n",
    "        self.memory = HierarchicalMemory()\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.memory.consolidate(X) \n",
    "\n",
    "        data = self.memory.get_all_data()\n",
    "        data = self.scaler.fit_transform(data)\n",
    "        X_tensor = torch.FloatTensor(data).to(self.device)\n",
    "        loader = DataLoader(X_tensor, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for _ in range(self.epochs):\n",
    "            for x_batch in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                x_hat, mu, logvar = self.model(x_batch)\n",
    "                \n",
    "                if (\n",
    "                    torch.isnan(mu).any() or\n",
    "                    torch.isnan(logvar).any() or\n",
    "                    torch.isnan(x_hat).any()\n",
    "                ):\n",
    "                    print(\"NaNs in model output! Skipping batch.\")\n",
    "                    continue\n",
    "                    \n",
    "                loss = elbo_loss(x_batch, x_hat, mu, logvar)\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(\"NaN or Inf detected in loss! Skipping batch.\")\n",
    "                    continue\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(self.device)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_hat, _, _ = self.model(X_tensor)\n",
    "            x_hat = torch.nan_to_num(x_hat, nan=0.0)  # Replace NaNs in reconstruction\n",
    "            loss = ((x_hat - X_tensor) ** 2).mean(dim=1)\n",
    "        return -loss.cpu().numpy()\n",
    "\n",
    "\n",
    "def scenario_design(normal_data, anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 to create a lifelong learning scenario.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (normal_concept, anomaly_concept) pairs forming the scenario.\n",
    "    \"\"\"\n",
    "    normal_concepts = create_phi(normal_data, c)\n",
    "    anomaly_concepts = create_gamma(anomaly_data, c)\n",
    "    \n",
    "    scenario = match_lambda(anomaly_concepts, normal_concepts)\n",
    "    \n",
    "    return scenario\n",
    "\n",
    "def evaluation_protocol(T, E, Y, model, strategy=\"naive\", replay_buffer_size=3000, memory_size=3000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2: Lifelong Learning Evaluation Protocol with multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        T (list): Sequence of N training sets.\n",
    "        E (list): Sequence of N testing sets.\n",
    "        Y (list): Sequence of true labels for test sets.\n",
    "        model (sklearn.base.BaseEstimator): A scikit-learn-like model instance that supports `fit` and `decision_function`.\n",
    "        strategy (str): Strategy for training.\n",
    "        replay_buffer_size (int): Maximum size of replay buffer if applicable\n",
    "\n",
    "    Returns:\n",
    "        numpy array: NxN results matrix R where R[i, j] is ROC-AUC of model on E[j] after learning T[i].\n",
    "    \"\"\"\n",
    "    N = len(T)\n",
    "    R = np.zeros((N, N))  \n",
    "\n",
    "    if strategy in [\"cumulative\"]:\n",
    "        cumulative_data = []\n",
    "    \n",
    "    if strategy in [\"replay\"]:\n",
    "        replay_buffer = []\n",
    "\n",
    "    if strategy == \"SSF\":\n",
    "        memory_buffer = None \n",
    "\n",
    "    \n",
    "    for i, Ti in tqdm(enumerate(T), desc=f\"Evaluating using {strategy} strategy\"):\n",
    "        current_model = deepcopy(model)\n",
    "\n",
    "        # -- NAIVE --\n",
    "        if strategy == \"naive\":\n",
    "            current_model.fit(Ti)\n",
    "\n",
    "        # -- CUMULATIVE --\n",
    "        if strategy == \"cumulative\":\n",
    "            cumulative_data.extend(Ti.tolist())\n",
    "            current_model.fit(np.array(cumulative_data)) \n",
    "\n",
    "        # -- REPLAY -- \n",
    "        if strategy == \"replay\":\n",
    "            if replay_buffer:\n",
    "                combined_data = np.vstack((np.array(replay_buffer), Ti))\n",
    "            else:\n",
    "                combined_data = Ti\n",
    "\n",
    "            current_model.fit(combined_data)\n",
    "\n",
    "            replay_buffer.extend(Ti.tolist())\n",
    "\n",
    "            if len(replay_buffer) > replay_buffer_size:\n",
    "                replay_buffer = replay_buffer[-replay_buffer_size:]\n",
    "        \n",
    "        # -- SSF -- \n",
    "        if strategy == \"SSF\":\n",
    "            if memory_buffer is None:\n",
    "                memory_buffer = Ti[:memory_size]\n",
    "            else:\n",
    "                if kolmogorov_smirnov_test(memory_buffer, Ti, alpha):\n",
    "                    selected = strategic_sample_selection(memory_buffer, Ti, top_k=1000)\n",
    "                    memory_buffer = update_memory_buffer(memory_buffer, selected, memory_size)\n",
    "            memory_buffer = np.unique(memory_buffer, axis=0)\n",
    "            current_model.fit(memory_buffer)\n",
    "\n",
    "        # -- VLAD --\n",
    "        if strategy == \"VLAD\":\n",
    "            current_model.fit(Ti)\n",
    "\n",
    "        # Eval\n",
    "        for j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) in enumerate(zip(E, Y)):\n",
    "            test_data = np.vstack((Ej_normal, Ej_anomaly))\n",
    "            test_labels = np.hstack((y_normal, y_anomaly))  \n",
    "        \n",
    "            scores = -current_model.decision_function(test_data)  \n",
    "            R[i, j] = average_precision_score(test_labels, scores)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8d81d-8bba-4559-920b-86f9d642c0c2",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55f3b00c-79f9-4d98-aa7c-0dd1cb98a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating normal concepts\n",
      "Finished creating anomaly concepts\n",
      "Finished matching concept pairs\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 5\n",
    "\n",
    "X_normal = X[y == 0]  \n",
    "X_anomaly = X[y == 1]\n",
    "\n",
    "normal_concepts = create_phi(X_normal, num_concepts)\n",
    "anomaly_concepts = create_gamma(X_anomaly, num_concepts)\n",
    "\n",
    "concept_pairs = match_lambda(anomaly_concepts, normal_concepts)\n",
    "\n",
    "T = []  \n",
    "E = [] \n",
    "Y = []\n",
    "\n",
    "for normal, anomaly in concept_pairs:\n",
    "\n",
    "    normal_train, normal_test = train_test_split(normal, test_size=0.3, random_state=42)\n",
    "    anomaly_train, anomaly_test = train_test_split(anomaly, test_size=0.3, random_state=42)  \n",
    "\n",
    "    T.append(normal_train)\n",
    "    E.append((normal_test, anomaly_test))\n",
    "\n",
    "    y_normal_test = np.zeros(len(normal_test))\n",
    "    y_anomaly_test = np.ones(len(anomaly_test))\n",
    "    \n",
    "    Y.append((y_normal_test, y_anomaly_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35516e86-3161-46cf-a3fe-2a7559327ca7",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa69b1c-a3fa-4d53-a0d6-62c8ed40e2bb",
   "metadata": {},
   "source": [
    "# VLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b466d199-541f-4c3d-8d40-98c5bda2dbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using VLAD strategy: 5it [01:27, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5755168211977642, BWT: -0.21587889577698816, FWT: 0.5316360701949371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_vlad = evaluation_protocol(T, E, Y, model=VLADWrapper(input_dim=X.shape[1]), strategy=\"VLAD\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_vlad)}, BWT: {BWT(R_vlad)}, FWT: {FWT(R_vlad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7913569-b154-4243-b660-df109cc2efcc",
   "metadata": {},
   "source": [
    "# SSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0316b5e2-f2b4-4ee9-95d3-ba51240a3546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 299.42it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 335.25it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 328.63it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 325.09it/s]\n",
      "Evaluating using SSF strategy: 5it [00:06,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5068922291086851, BWT: -0.04768689876118241, FWT: 0.49680187312341506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf_og = evaluation_protocol(T, E, Y, model=AEWrapper(input_dim=X.shape[1]), strategy=\"SSF\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf_og)}, BWT: {BWT(R_ssf_og)}, FWT: {FWT(R_ssf_og)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a110d25-bd8e-452a-9aa9-f0ed1110ff78",
   "metadata": {},
   "source": [
    "\n",
    "## LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dc23cbb-9f1e-4922-b6d6-30e38b6877e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aluating using SSF strategy: 0it [00:00, ?it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 273.05it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 256.08it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 276.87it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 326.53it/s]\n",
      "\n",
      "Evaluating using SSF strategy: 5it [00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.7544561432893502, BWT: -0.0039135441339650965, FWT: 0.45301001764958054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2407e687-65f9-4fd0-91ab-aeec46194150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:51, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.6207107852264637, BWT: -0.283093404759226, FWT: 0.377989401800772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f739cd8-a365-4473-93f7-afad06db7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [02:10, 26.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.9238354062988797, BWT: -0.004499087364359666, FWT: 0.17379610432039533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986be1f1-4345-463c-9e0c-d96d6965c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:53, 10.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.7034898501276438, BWT: -0.19004147760711937, FWT: 0.24881712704471132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea398ac8-335e-4d07-86bf-3f1d38fcb214",
   "metadata": {},
   "source": [
    "## IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f1f73a7-1ced-4b06-9d95-9ae0910d4276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aluating using SSF strategy: 0it [00:00, ?it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 281.14it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 253.28it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 237.31it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 244.61it/s]\n",
      "\n",
      "Evaluating using SSF strategy: 5it [00:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.6559285630296237, BWT: 0.06868127809581366, FWT: 0.804018261100736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y,  IsolationForest(n_estimators=100), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f202fe-3718-47e9-92e9-0c22128560ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:04,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.6771475708237422, BWT: -0.22528601715677166, FWT: 0.6957773121657185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, IsolationForest(n_estimators=100), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ce69456-76b9-4ad5-98d0-99dad36d3f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [00:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.753897896406868, BWT: -0.0066406078135951676, FWT: 0.7719174809547474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, IsolationForest(n_estimators=100), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c4cd01-9013-4206-a359-b81bd9efc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:05,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.6977725521692333, BWT: -0.16392284276343677, FWT: 0.7159532915927771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, IsolationForest(n_estimators=100), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518a2e23-e8aa-4cb3-821e-7136e982cd8b",
   "metadata": {},
   "source": [
    "## SGDOCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af0bd83b-8946-4170-a140-ab931a8948af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Aluating using SSF strategy: 0it [00:00, ?it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 288.13it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 307.55it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 306.85it/s]\n",
      "\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 271.76it/s]\n",
      "\n",
      "Evaluating using SSF strategy: 5it [00:02,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.790790866239933, BWT: -0.004091332504534584, FWT: 0.5467376505389003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y,  SGDOneClassSVM(), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dabcbd0-6f8b-4e08-b1f0-df8822d6fcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:00,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.6589862939199218, BWT: -0.2399821951513669, FWT: 0.5658554538516768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4ef0151-6f99-4f48-9025-c789ca69f474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [00:04,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.597799790906325, BWT: -0.215598947769658, FWT: 0.6579349113663381\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ae2708-dbfc-4dbd-ab56-9aedd7405b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:01,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.653901107789708, BWT: -0.23987138665874969, FWT: 0.5783463917929488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ed2c1-c826-4ea1-8f3f-85f6ac4eaa7f",
   "metadata": {},
   "source": [
    "# SLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "767548bb-cfce-4f23-a11a-03a5e88d58af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepod.models.slad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdeepod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mslad\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SLAD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepod.models.slad'"
     ]
    }
   ],
   "source": [
    "from deepod.models.slad import SLAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc35698c-92f2-46fb-a58b-6951f3dc6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y,  SGDOneClassSVM(), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d5060-780a-410d-9abd-f112e25f357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8572b-677d-4c67-bf2c-58713031e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
