{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c094fa-8da6-48b6-b19f-1627adfbfcba",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e802f5-4d04-43b3-ba66-19400b29a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from pyDeepInsight import ImageTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.base import clone\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765e6a2d-453f-4b10-8aa3-c4e8f516e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a1d38bf-fc37-42ff-a9a8-c5e35b2978b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "\n",
    "X_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "y = np.where(y == 6, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d0ff6-198a-4f43-bf2c-9c795817ad9e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fefa4ff-04b2-4dda-8b66-08c618fb7be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi(normal_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for normal data.\n",
    "    Uses k-Means clustering to partition normal data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        c (int): Number of desired normal concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of normal clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(normal_data)\n",
    "    \n",
    "    normal_concepts = [normal_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating normal concepts\")\n",
    "    \n",
    "    return normal_concepts\n",
    "\n",
    "\n",
    "def create_gamma(anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for anomaly data.\n",
    "    Uses k-Means clustering to partition anomaly data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired anomaly concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of anomaly clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(anomaly_data)\n",
    "    \n",
    "    anomaly_concepts = [anomaly_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating anomaly concepts\")\n",
    "    \n",
    "    return anomaly_concepts\n",
    "    \n",
    "def match_lambda(anomaly_concepts, normal_concepts):\n",
    "    \"\"\"\n",
    "    Matches each normal concept with the closest anomaly concept.\n",
    "    Uses Euclidean distance to determine the best match.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_concepts (list of numpy arrays): List of anomaly clusters.\n",
    "        normal_concepts (list of numpy arrays): List of normal clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Pairs of (normal_concept, matched_anomaly_concept)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    remaining_anomalies = anomaly_concepts.copy()\n",
    "\n",
    "    for normal_concept in normal_concepts:\n",
    "        normal_centroid = np.mean(normal_concept, axis=0)\n",
    "        anomaly_centroids = [np.mean(ac, axis=0) for ac in remaining_anomalies]\n",
    "\n",
    "        distances = cdist([normal_centroid], anomaly_centroids, metric='euclidean')[0]\n",
    "        closest_idx = np.argmin(distances)\n",
    "\n",
    "        pairs.append((normal_concept, remaining_anomalies[closest_idx]))\n",
    "        remaining_anomalies.pop(closest_idx)\n",
    "\n",
    "    print(\"Finished matching concept pairs\")\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def lifelong_roc_auc(R):\n",
    "    \"\"\"\n",
    "    Computes the Lifelong ROC-AUC metric.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN matrix of ROC-AUC scores, where R[i, j] is the model's \n",
    "                         performance on concept j after learning concept i.\n",
    "    \n",
    "    Returns:\n",
    "        float: Lifelong ROC-AUC score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    lower_triangular_sum = np.sum(np.tril(R))\n",
    "    normalization_factor = (N * (N + 1)) / 2\n",
    "\n",
    "    return lower_triangular_sum / normalization_factor\n",
    "\n",
    "def BWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Backward Transfer (BWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: BWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    backward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            backward_transfer += (R[i, j] - R[j, j])\n",
    "            count += 1\n",
    "\n",
    "    return backward_transfer / count if count > 0 else 0\n",
    "\n",
    "def FWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Forward Transfer (FWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: FWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    forward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): \n",
    "            forward_transfer += R[i, j]\n",
    "            count += 1\n",
    "\n",
    "    return forward_transfer / count if count > 0 else 0 \n",
    "\n",
    "def scenario_design(normal_data, anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 to create a lifelong learning scenario.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (normal_concept, anomaly_concept) pairs forming the scenario.\n",
    "    \"\"\"\n",
    "    normal_concepts = create_phi(normal_data, c)\n",
    "    anomaly_concepts = create_gamma(anomaly_data, c)\n",
    "    \n",
    "    scenario = match_lambda(anomaly_concepts, normal_concepts)\n",
    "    \n",
    "    return scenario\n",
    "\n",
    "def evaluation_protocol(T, E, Y, model, strategy=\"naive\", replay_buffer_size=3000):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2: Lifelong Learning Evaluation Protocol with multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        T (list): Sequence of N training sets.\n",
    "        E (list): Sequence of N testing sets.\n",
    "        Y (list): Sequence of true labels for test sets.\n",
    "        model (sklearn.base.BaseEstimator): A scikit-learn-like model instance that supports `fit` and `decision_function`.\n",
    "        strategy (str): Strategy for training.\n",
    "        replay_buffer_size (int): Maximum size of replay buffer (only used if strategy=\"replay\").\n",
    "\n",
    "    Returns:\n",
    "        numpy array: NxN results matrix R where R[i, j] is ROC-AUC of model on E[j] after learning T[i].\n",
    "    \"\"\"\n",
    "    N = len(T)\n",
    "    R = np.zeros((N, N))  \n",
    "\n",
    "    if strategy in [\"cumulative\"]:\n",
    "        cumulative_data = []\n",
    "    \n",
    "    if strategy in [\"replay\"]:\n",
    "        replay_buffer = []\n",
    "\n",
    "    \n",
    "    for i, Ti in tqdm(enumerate(T), desc=f\"Evaluating using {strategy} strategy\"):\n",
    "        current_model = clone(model)\n",
    "\n",
    "        # -- NAIVE --\n",
    "        if strategy == \"naive\":\n",
    "            current_model.fit(Ti)\n",
    "\n",
    "        # -- CUMULATIVE --\n",
    "        elif strategy == \"cumulative\":\n",
    "            cumulative_data.extend(Ti.tolist())\n",
    "            current_model.fit(np.array(cumulative_data)) \n",
    "\n",
    "        # -- REPLAY -- \n",
    "        elif strategy == \"replay\":\n",
    "            if replay_buffer:\n",
    "                combined_data = np.vstack((np.array(replay_buffer), Ti))\n",
    "            else:\n",
    "                combined_data = Ti\n",
    "\n",
    "            current_model.fit(combined_data)\n",
    "\n",
    "            replay_buffer.extend(Ti.tolist())\n",
    "\n",
    "            if len(replay_buffer) > replay_buffer_size:\n",
    "                replay_buffer = replay_buffer[-replay_buffer_size:]\n",
    "\n",
    "        # Eval\n",
    "        for j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) in enumerate(zip(E, Y)):\n",
    "            test_data = np.vstack((Ej_normal, Ej_anomaly))\n",
    "            test_labels = np.hstack((y_normal, y_anomaly))  \n",
    "        \n",
    "            scores = -current_model.decision_function(test_data)  \n",
    "            R[i, j] = roc_auc_score(test_labels, scores)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf8d81d-8bba-4559-920b-86f9d642c0c2",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55f3b00c-79f9-4d98-aa7c-0dd1cb98a783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating normal concepts\n",
      "Finished creating anomaly concepts\n",
      "Finished matching concept pairs\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 5\n",
    "\n",
    "X_normal = X[y == 0]  \n",
    "X_anomaly = X[y == 1]\n",
    "\n",
    "normal_concepts = create_phi(X_normal, num_concepts)\n",
    "anomaly_concepts = create_gamma(X_anomaly, num_concepts)\n",
    "\n",
    "concept_pairs = match_lambda(anomaly_concepts, normal_concepts)\n",
    "\n",
    "T = []  \n",
    "E = [] \n",
    "Y = []\n",
    "\n",
    "for normal, anomaly in concept_pairs:\n",
    "\n",
    "    normal_train, normal_test = train_test_split(normal, test_size=0.3, random_state=42)\n",
    "    anomaly_train, anomaly_test = train_test_split(anomaly, test_size=0.3, random_state=42)  \n",
    "\n",
    "    T.append(normal_train)\n",
    "    E.append((normal_test, anomaly_test))\n",
    "\n",
    "    y_normal_test = np.zeros(len(normal_test))\n",
    "    y_anomaly_test = np.ones(len(anomaly_test))\n",
    "    \n",
    "    Y.append((y_normal_test, y_anomaly_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35516e86-3161-46cf-a3fe-2a7559327ca7",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2407e687-65f9-4fd0-91ab-aeec46194150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:05,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5955825367666304, BWT: -0.5674660688436939, FWT: 0.5068781194027958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f739cd8-a365-4473-93f7-afad06db7593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [00:19,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.889691385111648, BWT: -0.06288058943392247, FWT: 0.5746263747412855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986be1f1-4345-463c-9e0c-d96d6965c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:07,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.7480611601487391, BWT: -0.269852002077916, FWT: 0.582651759230246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
