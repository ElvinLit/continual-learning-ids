{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421755df-c4c3-4adb-b0c1-74c5910777b9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e609f474-dd80-40fd-b456-a953f6253fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from pyDeepInsight import ImageTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4fad6262-cb13-420b-9972-45fd7445347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "49e23bcc-3c21-48cb-aa68-49dcd56a5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "\n",
    "X_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284b677-4c43-4683-a7f1-aa66558bafca",
   "metadata": {},
   "source": [
    "## PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "52457411-d0c2-40dc-9cf0-2c9577c2c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_feature_selection(X, k, explained_variance_threshold=0.95):\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumulative_variance >= explained_variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "\n",
    "    feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "    top_k_indices = np.argsort(feature_importance)[-k:]\n",
    "    \n",
    "    return top_k_indices\n",
    "\n",
    "k = 31\n",
    "top_features_indices = pca_feature_selection(X_train, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3efcf97d-9907-4068-add2-bfff041a62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == 6, 0, 1)\n",
    "y_test = np.where(y_test == 6, 0, 1)\n",
    "y_val = np.where(y_val == 6, 0, 1)\n",
    "\n",
    "X_train_selected = X_train[:, top_features_indices]\n",
    "X_test_selected = X_test[:, top_features_indices]\n",
    "X_val_selected = X_val[:, top_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da4b62-4d45-45f9-8e5f-811b0a72beaa",
   "metadata": {},
   "source": [
    "## PyDeepInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "6d68ffeb-726c-4083-a717-80d2f9c4ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = ImageTransformer(\n",
    "    pixels=8,\n",
    "    feature_extractor='tsne',\n",
    "    discretization='lsa'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "63306b2e-663f-4b3e-b1cb-2418408043ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "it.fit(X_train_selected)\n",
    "X_train_images = it.transform(X_train_selected, 'pytorch')\n",
    "\n",
    "X_test_images = it.transform(X_test_selected, 'pytorch')\n",
    "\n",
    "X_val_images = it.transform(X_val_selected, 'pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4288166-7929-4afd-bc24-c2ecf3659c72",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "e553d1d9-3334-4a4f-a0a9-f9754f7d699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, channels, img_height, img_width = X_train_images.shape\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "08acf3e3-f1e1-4a8b-8e87-49aae4b2a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # Output: (batch_size, 16, 8, 8)\n",
    "        x = self.pool(x)              # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.relu(self.conv2(x))  # Output: (batch_size, 32, 4, 4)\n",
    "        x = self.pool(x)              # Output: (batch_size, 32, 2, 2)\n",
    "        x = x.view(x.size(0), -1)     # Flatten to (batch_size, 128)\n",
    "        x = self.fc1(x)               # Output: (batch_size, feature_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "149b7000-9c0d-4e55-9224-532d6d307ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(feature_dim, 32 * 2 * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16, img_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc2(z))           # Output: (batch_size, 128)\n",
    "        x = x.view(x.size(0), 32, 2, 2)      # Reshape to (batch_size, 32, 2, 2)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 32, 4, 4)\n",
    "        x = self.relu(self.deconv1(x))       # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 16, 8, 8)\n",
    "        x = self.sigmoid(self.deconv2(x))    # Output: (batch_size, img_channels, 8, 8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f3ddbeb3-0fe5-4862-bd86-128ee846cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(MAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, feature_dim, latent_dim)\n",
    "        self.decoder = Decoder(img_channels, feature_dim)\n",
    "\n",
    "    def mask_input(self, x, mask_ratio=0.25):\n",
    "        # Generate a mask with 0s and 1s, keeping only (1-mask_ratio) of the original input\n",
    "        mask = torch.rand(x.shape, device=x.device) > mask_ratio\n",
    "        x_masked = x * mask\n",
    "        return x_masked, mask\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.25):\n",
    "        x_masked, mask = self.mask_input(x, mask_ratio)  # Apply masking to input\n",
    "        z = self.encoder(x_masked)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c3553c18-5547-4922-9e60-ee812261ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss_function(reconstructed, original, mask):\n",
    "    # Only calculate reconstruction loss on the masked parts\n",
    "    masked_original = original * mask\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, masked_original, reduction='sum')\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "bc0610c6-6ec6-451d-8953-808a20c67094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MAE(img_channels=3, feature_dim=32, latent_dim=16).to(device)\n",
    "model.load_state_dict(torch.load(\"deepinsight_mae_normal.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3ea0d9b8-1d4d-492c-8dae-0d8ec5347ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_340/740995774.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_normal_tensor = torch.tensor(X_train_normal, dtype=torch.float32)\n",
      "/tmp/ipykernel_340/740995774.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_anomaly_tensor = torch.tensor(X_train_anomaly, dtype=torch.float32)\n",
      "/tmp/ipykernel_340/740995774.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test_images, dtype=torch.float32)\n",
      "/tmp/ipykernel_340/740995774.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_tensor = torch.tensor(X_val_images, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "normal_indices = np.where(y_train == 0)[0]\n",
    "X_train_normal = X_train_images[normal_indices]\n",
    "y_train_normal = y_train[normal_indices]\n",
    "\n",
    "anomaly_indices = np.where(y_train == 1)[0]\n",
    "X_train_anomaly = X_train_images[anomaly_indices]\n",
    "y_train_anomaly = y_train[anomaly_indices]\n",
    "\n",
    "X_train_normal_tensor = torch.tensor(X_train_normal, dtype=torch.float32)\n",
    "X_train_anomaly_tensor = torch.tensor(X_train_anomaly, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_images, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_images, dtype=torch.float32)\n",
    "\n",
    "train_normal_dataset = TensorDataset(X_train_normal_tensor)\n",
    "train_anomaly_dataset = TensorDataset(X_train_anomaly_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.long))\n",
    "val_dataset = TensorDataset(X_val_tensor, torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "batch_size = 32 \n",
    "train_normal_loader = DataLoader(train_normal_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_anomaly_loader = DataLoader(train_anomaly_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b2b4a812-cc94-4fc1-8502-d3ca53c82a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_features(model, data_loader, device='cuda'):\n",
    "    model.eval() \n",
    "    latent_features = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, total=len(data_loader), desc=\"Extracting features\"):\n",
    "            if len(batch) == 2:\n",
    "                data, _ = batch  \n",
    "            else:\n",
    "                (data,) = batch  \n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            latent_feature = model.encoder(data)\n",
    "            latent_features.append(latent_feature.cpu().numpy())\n",
    "\n",
    "    latent_features = np.concatenate(latent_features, axis=0)\n",
    "    \n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "ebc25f40-aa67-4baa-9a26-31b03be30433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1860/1860 [00:01<00:00, 1407.34it/s]\n",
      "Extracting features: 100%|██████████| 3294/3294 [00:02<00:00, 1414.27it/s]\n",
      "Extracting features: 100%|██████████| 1289/1289 [00:01<00:00, 1180.63it/s]\n",
      "Extracting features: 100%|██████████| 1611/1611 [00:01<00:00, 1167.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train_normal_latent_features = extract_latent_features(model, train_normal_loader, device)\n",
    "train_anomaly_latent_features = extract_latent_features(model, train_anomaly_loader, device)\n",
    "val_latent_features = extract_latent_features(model, val_loader, device)\n",
    "test_latent_features = extract_latent_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7c47a3ee-fdbf-40d3-995a-76fdf71e8879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59520, 32), (59520,))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normal_latent_features.shape, y_train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "c6dadf0e-c93e-43f5-a87c-1d9b08159d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105390, 32), (105390,))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_anomaly_latent_features.shape, y_train_anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "00853abd-0936-44e2-83f0-f7fb56614a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41228, 32), (41228,))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent_features.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "03c81533-def3-4429-9b7b-b11b2e3f9b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51535, 32), (51535,))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_latent_features.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1538386-1a40-44be-aaed-47aa12856b72",
   "metadata": {},
   "source": [
    "# Important Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488a0f3-04fa-47ed-b2dc-2bb4ea2f8211",
   "metadata": {},
   "source": [
    "- SAFE has 4 modules. We are assuming that the MAE is pre-trained already and we have already used the encoder head to extract latent features. We are implementing this on the LOF in module 4 (novelty detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b5b75-554e-4024-98a9-9c96e42f455b",
   "metadata": {},
   "source": [
    "## Creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e6f992c3-e4de-49be-bfac-47bdce14e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi(normal_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for normal data.\n",
    "    Uses k-Means clustering to partition normal data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        c (int): Number of desired normal concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of normal clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(normal_data)\n",
    "    \n",
    "    normal_concepts = [normal_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating normal concepts\")\n",
    "    \n",
    "    return normal_concepts\n",
    "\n",
    "\n",
    "def create_gamma(anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for anomaly data.\n",
    "    Uses k-Means clustering to partition anomaly data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired anomaly concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of anomaly clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(anomaly_data)\n",
    "    \n",
    "    anomaly_concepts = [anomaly_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating anomaly concepts\")\n",
    "    \n",
    "    return anomaly_concepts\n",
    "    \n",
    "def match_lambda(anomaly_concepts, normal_concepts):\n",
    "    \"\"\"\n",
    "    Matches each normal concept with the closest anomaly concept.\n",
    "    Uses Euclidean distance to determine the best match.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_concepts (list of numpy arrays): List of anomaly clusters.\n",
    "        normal_concepts (list of numpy arrays): List of normal clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Pairs of (normal_concept, matched_anomaly_concept)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    remaining_anomalies = anomaly_concepts.copy()\n",
    "\n",
    "    for normal_concept in normal_concepts:\n",
    "        normal_centroid = np.mean(normal_concept, axis=0)\n",
    "        anomaly_centroids = [np.mean(ac, axis=0) for ac in remaining_anomalies]\n",
    "\n",
    "        distances = cdist([normal_centroid], anomaly_centroids, metric='euclidean')[0]\n",
    "        closest_idx = np.argmin(distances)\n",
    "\n",
    "        pairs.append((normal_concept, remaining_anomalies[closest_idx]))\n",
    "        remaining_anomalies.pop(closest_idx)\n",
    "\n",
    "    print(\"Finished matching concept pairs\")\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d08c06-732c-4eac-9e63-cfcd3691290a",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5f9bdeb1-c8e5-41da-a84b-f166c1cc130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifelong_roc_auc(R):\n",
    "    \"\"\"\n",
    "    Computes the Lifelong ROC-AUC metric.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN matrix of ROC-AUC scores, where R[i, j] is the model's \n",
    "                         performance on concept j after learning concept i.\n",
    "    \n",
    "    Returns:\n",
    "        float: Lifelong ROC-AUC score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    lower_triangular_sum = np.sum(np.tril(R))\n",
    "    normalization_factor = (N * (N + 1)) / 2\n",
    "\n",
    "    return lower_triangular_sum / normalization_factor\n",
    "\n",
    "def BWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Backward Transfer (BWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: BWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    backward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            backward_transfer += (R[i, j] - R[j, j])\n",
    "            count += 1\n",
    "\n",
    "    return backward_transfer / count if count > 0 else 0\n",
    "\n",
    "def FWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Forward Transfer (FWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: FWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    forward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): \n",
    "            forward_transfer += R[i, j]\n",
    "            count += 1\n",
    "\n",
    "    return forward_transfer / count if count > 0 else 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf727f9-dbc5-4216-8a1b-4fadc5419337",
   "metadata": {},
   "source": [
    "## Scenario Design + Evaluation Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d267b4db-ae8f-4c28-b0f9-4f9a580a70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_design(normal_data, anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 to create a lifelong learning scenario.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (normal_concept, anomaly_concept) pairs forming the scenario.\n",
    "    \"\"\"\n",
    "    normal_concepts = create_phi(normal_data, c)\n",
    "    anomaly_concepts = create_gamma(anomaly_data, c)\n",
    "    \n",
    "    scenario = match_lambda(anomaly_concepts, normal_concepts)\n",
    "    \n",
    "    return scenario\n",
    "\n",
    "def evaluation_protocol(T, E, Y, strategy=\"naive\", replay_buffer_size=3000):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2: Lifelong Learning Evaluation Protocol with multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        T (list): Sequence of N training sets.\n",
    "        E (list): Sequence of N testing sets.\n",
    "        Y (list): Sequence of true labels for test sets.\n",
    "        strategy (str): Strategy for training (\"naive\", \"mste\", \"replay\").\n",
    "        replay_buffer_size (int): Maximum size of replay buffer (only used if strategy=\"replay\").\n",
    "\n",
    "    Returns:\n",
    "        numpy array: NxN results matrix R where R[i, j] is ROC-AUC of model on E[j] after learning T[i].\n",
    "    \"\"\"\n",
    "    N = len(T)\n",
    "    R = np.zeros((N, N))  \n",
    "\n",
    "    if strategy == \"mste\":\n",
    "        experts = []\n",
    "    \n",
    "    elif strategy == \"replay\":\n",
    "        replay_buffer = []\n",
    "\n",
    "    \n",
    "    for i, Ti in tqdm(enumerate(T), desc=f\"Evaluating using {strategy} strategy\"):\n",
    "        if strategy == \"naive\":\n",
    "            model = LocalOutlierFactor(n_neighbors=20, novelty=True)  \n",
    "            model.fit(Ti)\n",
    "\n",
    "        \"\"\"\n",
    "        elif strategy == \"mste\":\n",
    "            new_model = LocalOutlierFactor(n_neighbors=20, novelty=True)  \n",
    "            new_model.fit(Ti)\n",
    "            experts.append(new_model)\n",
    "        \"\"\"\n",
    "        \n",
    "        elif strategy == \"replay\":\n",
    "            if replay_buffer:\n",
    "                combined_data = np.vstack((np.array(replay_buffer), Ti))\n",
    "            else:\n",
    "                combined_data = Ti\n",
    "\n",
    "            model = LocalOutlierFactor(n_neighbors=20, novelty=True)  \n",
    "            model.fit(combined_data)\n",
    "\n",
    "            replay_buffer.extend(Ti)\n",
    "            \n",
    "            if len(replay_buffer) > replay_buffer_size:\n",
    "                replay_buffer = replay_buffer[-replay_buffer_size:]\n",
    "\n",
    "        for j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) in enumerate(zip(E, Y)):\n",
    "           \n",
    "            test_data = np.vstack((Ej_normal, Ej_anomaly))\n",
    "            test_labels = np.hstack((y_normal, y_anomaly))  \n",
    "        \n",
    "            scores = -model.decision_function(test_data)  \n",
    "            R[i, j] = roc_auc_score(test_labels, scores)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461297b-598d-4ea0-9e43-5ba502595a3e",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20004cfd-6859-4f18-be74-ee20746809d3",
   "metadata": {},
   "source": [
    "### First concatenate all our X_ and y data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c279bf9e-5173-4273-ab10-d9cd687efae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_normal_latent_features, train_anomaly_latent_features, val_latent_features, test_latent_features))\n",
    "y = np.hstack((y_train_normal, y_train_anomaly, y_val, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750ca1a-c38b-4396-86e2-de703f2c6636",
   "metadata": {},
   "source": [
    "### Create 'c' normal/anomaly concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0dd6f88e-25a2-4669-a452-7e949e9243fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating normal concepts\n",
      "Finished creating anomaly concepts\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 5\n",
    "\n",
    "X_normal = X[y == 0]  \n",
    "X_anomaly = X[y == 1]\n",
    "\n",
    "normal_concepts = create_phi(X_normal, num_concepts)\n",
    "anomaly_concepts = create_gamma(X_anomaly, num_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1edae2-78bc-4506-8c4b-ba8ac3521a22",
   "metadata": {},
   "source": [
    "### Use lambda function to pair normal/anomaly concepts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "64e97074-165e-46f9-be2c-a315b929dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished matching concept pairs\n"
     ]
    }
   ],
   "source": [
    "concept_pairs = match_lambda(anomaly_concepts, normal_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187e215-35f5-458e-a743-7805800d97fe",
   "metadata": {},
   "source": [
    "### Creating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "16d85d56-1f4e-428e-8b25-3ccf46b205f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []  \n",
    "E = [] \n",
    "Y = []\n",
    "\n",
    "for normal, anomaly in concept_pairs:\n",
    "\n",
    "    normal_train, normal_test = train_test_split(normal, test_size=0.3, random_state=42)\n",
    "    anomaly_train, anomaly_test = train_test_split(anomaly, test_size=0.3, random_state=42)  \n",
    "\n",
    "    T.append(normal_train)\n",
    "    E.append((normal_test, anomaly_test))\n",
    "\n",
    "    y_normal_test = np.zeros(len(normal_test))\n",
    "    y_anomaly_test = np.ones(len(anomaly_test))\n",
    "    \n",
    "    Y.append((y_normal_test, y_anomaly_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "8ca1c950-b34e-481b-b77f-f38d5f5bb3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnormal_concepts = create_phi(train_normal_latent_features, num_concepts)\\n#anomaly_concepts = create_gamma(train_anomaly_latent_features, num_concepts)\\n#concept_pairs = match_lambda(anomaly_concepts, normal_concepts)\\n\\nT = normal_concepts\\n\\nE_normal = create_phi(test_latent_features, num_concepts)\\nE_anomaly = create_gamma(test_latent_features, num_concepts)   \\n\\nY_normal = [y_test[:len(normal)] for normal in E_normal]  \\nY_anomaly = [y_test[:len(anomaly)] for anomaly in E_anomaly]  \\n\\nE = [(normal, anomaly) for normal, anomaly in zip(E_normal, E_anomaly)]\\nY = [(y_normal, y_anomaly) for y_normal, y_anomaly in zip(Y_normal, Y_anomaly)]\\n'"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "normal_concepts = create_phi(train_normal_latent_features, num_concepts)\n",
    "#anomaly_concepts = create_gamma(train_anomaly_latent_features, num_concepts)\n",
    "#concept_pairs = match_lambda(anomaly_concepts, normal_concepts)\n",
    "\n",
    "T = normal_concepts\n",
    "\n",
    "E_normal = create_phi(test_latent_features, num_concepts)\n",
    "E_anomaly = create_gamma(test_latent_features, num_concepts)   \n",
    "\n",
    "Y_normal = [y_test[:len(normal)] for normal in E_normal]  \n",
    "Y_anomaly = [y_test[:len(anomaly)] for anomaly in E_anomaly]  \n",
    "\n",
    "E = [(normal, anomaly) for normal, anomaly in zip(E_normal, E_anomaly)]\n",
    "Y = [(y_normal, y_anomaly) for y_normal, y_anomaly in zip(Y_normal, Y_anomaly)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "482f294f-f5b7-42db-808d-e2848f5d7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:06,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5209079981103106, BWT: -0.598555385482603, FWT: 0.5675865361608492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "1769eb0c-96b2-4c1b-b438-12e0ddf0050b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nR_mste = evaluation_protocol(T, E, Y, strategy=\"mste\")\\nprint(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_mste)}, BWT: {BWT(R_mste)}, FWT: {FWT(R_mste)}\")\\n'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "R_mste = evaluation_protocol(T, E, Y, strategy=\"mste\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_mste)}, BWT: {BWT(R_mste)}, FWT: {FWT(R_mste)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b4044123-b410-4198-b4a2-75604b283c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:07,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.7456418835718264, BWT: -0.24209173663826164, FWT: 0.4489903474284862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, strategy=\"replay\", replay_buffer_size=3000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dce447-7bbc-4e65-b91e-b359983c88e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
