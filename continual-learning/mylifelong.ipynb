{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421755df-c4c3-4adb-b0c1-74c5910777b9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e609f474-dd80-40fd-b456-a953f6253fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from pyDeepInsight import ImageTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.base import clone\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad6262-cb13-420b-9972-45fd7445347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e23bcc-3c21-48cb-aa68-49dcd56a5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "\n",
    "X_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')\n",
    "\n",
    "X = np.concatenate([X_train, X_test], axis=0)\n",
    "y = np.concatenate([y_train, y_test], axis=0)\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.2, random_state=42, stratify=y_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284b677-4c43-4683-a7f1-aa66558bafca",
   "metadata": {},
   "source": [
    "## PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52457411-d0c2-40dc-9cf0-2c9577c2c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_feature_selection(X, k, explained_variance_threshold=0.95):\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumulative_variance >= explained_variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "\n",
    "    feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "    top_k_indices = np.argsort(feature_importance)[-k:]\n",
    "    \n",
    "    return top_k_indices\n",
    "\n",
    "k = 31\n",
    "top_features_indices = pca_feature_selection(X_train, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3efcf97d-9907-4068-add2-bfff041a62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == 11, 0, 1)\n",
    "y_test = np.where(y_test == 11, 0, 1)\n",
    "y_val = np.where(y_val == 11, 0, 1)\n",
    "\n",
    "X_train_selected = X_train[:, top_features_indices]\n",
    "X_test_selected = X_test[:, top_features_indices]\n",
    "X_val_selected = X_val[:, top_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da4b62-4d45-45f9-8e5f-811b0a72beaa",
   "metadata": {},
   "source": [
    "## PyDeepInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d68ffeb-726c-4083-a717-80d2f9c4ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = ImageTransformer(\n",
    "    pixels=8,\n",
    "    feature_extractor='tsne',\n",
    "    discretization='lsa'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63306b2e-663f-4b3e-b1cb-2418408043ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "it.fit(X_train_selected)\n",
    "X_train_images = it.transform(X_train_selected, 'pytorch')\n",
    "\n",
    "X_test_images = it.transform(X_test_selected, 'pytorch')\n",
    "\n",
    "X_val_images = it.transform(X_val_selected, 'pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4288166-7929-4afd-bc24-c2ecf3659c72",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e553d1d9-3334-4a4f-a0a9-f9754f7d699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, channels, img_height, img_width = X_train_images.shape\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08acf3e3-f1e1-4a8b-8e87-49aae4b2a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # Output: (batch_size, 16, 8, 8)\n",
    "        x = self.pool(x)              # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.relu(self.conv2(x))  # Output: (batch_size, 32, 4, 4)\n",
    "        x = self.pool(x)              # Output: (batch_size, 32, 2, 2)\n",
    "        x = x.view(x.size(0), -1)     # Flatten to (batch_size, 128)\n",
    "        x = self.fc1(x)               # Output: (batch_size, feature_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149b7000-9c0d-4e55-9224-532d6d307ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(feature_dim, 32 * 2 * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16, img_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc2(z))           # Output: (batch_size, 128)\n",
    "        x = x.view(x.size(0), 32, 2, 2)      # Reshape to (batch_size, 32, 2, 2)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 32, 4, 4)\n",
    "        x = self.relu(self.deconv1(x))       # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 16, 8, 8)\n",
    "        x = self.sigmoid(self.deconv2(x))    # Output: (batch_size, img_channels, 8, 8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3ddbeb3-0fe5-4862-bd86-128ee846cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(MAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, feature_dim, latent_dim)\n",
    "        self.decoder = Decoder(img_channels, feature_dim)\n",
    "\n",
    "    def mask_input(self, x, mask_ratio=0.25):\n",
    "        # Generate a mask with 0s and 1s, keeping only (1-mask_ratio) of the original input\n",
    "        mask = torch.rand(x.shape, device=x.device) > mask_ratio\n",
    "        x_masked = x * mask\n",
    "        return x_masked, mask\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.25):\n",
    "        x_masked, mask = self.mask_input(x, mask_ratio)  # Apply masking to input\n",
    "        z = self.encoder(x_masked)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3553c18-5547-4922-9e60-ee812261ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss_function(reconstructed, original, mask):\n",
    "    # Only calculate reconstruction loss on the masked parts\n",
    "    masked_original = original * mask\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, masked_original, reduction='sum')\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0610c6-6ec6-451d-8953-808a20c67094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MAE(img_channels=3, feature_dim=32, latent_dim=16).to(device)\n",
    "model.load_state_dict(torch.load(\"deepinsight_mae_normal.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea0d9b8-1d4d-492c-8dae-0d8ec5347ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_319/740995774.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_normal_tensor = torch.tensor(X_train_normal, dtype=torch.float32)\n",
      "/tmp/ipykernel_319/740995774.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train_anomaly_tensor = torch.tensor(X_train_anomaly, dtype=torch.float32)\n",
      "/tmp/ipykernel_319/740995774.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test_tensor = torch.tensor(X_test_images, dtype=torch.float32)\n",
      "/tmp/ipykernel_319/740995774.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_val_tensor = torch.tensor(X_val_images, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "normal_indices = np.where(y_train == 0)[0]\n",
    "X_train_normal = X_train_images[normal_indices]\n",
    "y_train_normal = y_train[normal_indices]\n",
    "\n",
    "anomaly_indices = np.where(y_train == 1)[0]\n",
    "X_train_anomaly = X_train_images[anomaly_indices]\n",
    "y_train_anomaly = y_train[anomaly_indices]\n",
    "\n",
    "X_train_normal_tensor = torch.tensor(X_train_normal, dtype=torch.float32)\n",
    "X_train_anomaly_tensor = torch.tensor(X_train_anomaly, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_images, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_images, dtype=torch.float32)\n",
    "\n",
    "train_normal_dataset = TensorDataset(X_train_normal_tensor)\n",
    "train_anomaly_dataset = TensorDataset(X_train_anomaly_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.tensor(y_test, dtype=torch.long))\n",
    "val_dataset = TensorDataset(X_val_tensor, torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "batch_size = 32 \n",
    "train_normal_loader = DataLoader(train_normal_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_anomaly_loader = DataLoader(train_anomaly_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b4a812-cc94-4fc1-8502-d3ca53c82a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_features(model, data_loader, device='cuda'):\n",
    "    model.eval() \n",
    "    latent_features = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, total=len(data_loader), desc=\"Extracting features\"):\n",
    "            if len(batch) == 2:\n",
    "                data, _ = batch  \n",
    "            else:\n",
    "                (data,) = batch  \n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            latent_feature = model.encoder(data)\n",
    "            latent_features.append(latent_feature.cpu().numpy())\n",
    "\n",
    "    latent_features = np.concatenate(latent_features, axis=0)\n",
    "    \n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebc25f40-aa67-4baa-9a26-31b03be30433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 6825/6825 [00:04<00:00, 1510.81it/s]\n",
      "Extracting features: 100%|██████████| 4956/4956 [00:02<00:00, 2195.52it/s]\n",
      "Extracting features: 100%|██████████| 2945/2945 [00:01<00:00, 1856.41it/s]\n",
      "Extracting features: 100%|██████████| 3682/3682 [00:01<00:00, 1871.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_normal_latent_features = extract_latent_features(model, train_normal_loader, device)\n",
    "train_anomaly_latent_features = extract_latent_features(model, train_anomaly_loader, device)\n",
    "val_latent_features = extract_latent_features(model, val_loader, device)\n",
    "test_latent_features = extract_latent_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c47a3ee-fdbf-40d3-995a-76fdf71e8879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((218369, 32), (218369,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_normal_latent_features.shape, y_train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6dadf0e-c93e-43f5-a87c-1d9b08159d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158568, 32), (158568,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_anomaly_latent_features.shape, y_train_anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00853abd-0936-44e2-83f0-f7fb56614a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((94235, 32), (94235,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_latent_features.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03c81533-def3-4429-9b7b-b11b2e3f9b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117793, 32), (117793,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_latent_features.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1538386-1a40-44be-aaed-47aa12856b72",
   "metadata": {},
   "source": [
    "# Important Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488a0f3-04fa-47ed-b2dc-2bb4ea2f8211",
   "metadata": {},
   "source": [
    "- SAFE has 4 modules. We are assuming that the MAE is pre-trained already and we have already used the encoder head to extract latent features. We are implementing this on the LOF in module 4 (novelty detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b5b75-554e-4024-98a9-9c96e42f455b",
   "metadata": {},
   "source": [
    "## Creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6f992c3-e4de-49be-bfac-47bdce14e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi(normal_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for normal data.\n",
    "    Uses k-Means clustering to partition normal data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        c (int): Number of desired normal concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of normal clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(normal_data)\n",
    "    \n",
    "    normal_concepts = [normal_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating normal concepts\")\n",
    "    \n",
    "    return normal_concepts\n",
    "\n",
    "\n",
    "def create_gamma(anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for anomaly data.\n",
    "    Uses k-Means clustering to partition anomaly data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired anomaly concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of anomaly clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(anomaly_data)\n",
    "    \n",
    "    anomaly_concepts = [anomaly_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating anomaly concepts\")\n",
    "    \n",
    "    return anomaly_concepts\n",
    "    \n",
    "def match_lambda(anomaly_concepts, normal_concepts):\n",
    "    \"\"\"\n",
    "    Matches each normal concept with the closest anomaly concept.\n",
    "    Uses Euclidean distance to determine the best match.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_concepts (list of numpy arrays): List of anomaly clusters.\n",
    "        normal_concepts (list of numpy arrays): List of normal clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Pairs of (normal_concept, matched_anomaly_concept)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    remaining_anomalies = anomaly_concepts.copy()\n",
    "\n",
    "    for normal_concept in normal_concepts:\n",
    "        normal_centroid = np.mean(normal_concept, axis=0)\n",
    "        anomaly_centroids = [np.mean(ac, axis=0) for ac in remaining_anomalies]\n",
    "\n",
    "        distances = cdist([normal_centroid], anomaly_centroids, metric='euclidean')[0]\n",
    "        closest_idx = np.argmin(distances)\n",
    "\n",
    "        pairs.append((normal_concept, remaining_anomalies[closest_idx]))\n",
    "        remaining_anomalies.pop(closest_idx)\n",
    "\n",
    "    print(\"Finished matching concept pairs\")\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d08c06-732c-4eac-9e63-cfcd3691290a",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f9bdeb1-c8e5-41da-a84b-f166c1cc130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifelong_roc_auc(R):\n",
    "    \"\"\"\n",
    "    Computes the Lifelong ROC-AUC metric.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN matrix of ROC-AUC scores, where R[i, j] is the model's \n",
    "                         performance on concept j after learning concept i.\n",
    "    \n",
    "    Returns:\n",
    "        float: Lifelong ROC-AUC score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    lower_triangular_sum = np.sum(np.tril(R))\n",
    "    normalization_factor = (N * (N + 1)) / 2\n",
    "\n",
    "    return lower_triangular_sum / normalization_factor\n",
    "\n",
    "def BWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Backward Transfer (BWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: BWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    backward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            backward_transfer += (R[i, j] - R[j, j])\n",
    "            count += 1\n",
    "\n",
    "    return backward_transfer / count if count > 0 else 0\n",
    "\n",
    "def FWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Forward Transfer (FWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: FWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    forward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): \n",
    "            forward_transfer += R[i, j]\n",
    "            count += 1\n",
    "\n",
    "    return forward_transfer / count if count > 0 else 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669576fe-cbe8-4f1e-918b-e2d4ed5bfadb",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "71703029-8cad-4317-be17-d74f2a4a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kolmogorov_smirnov_test(X_old, X_new, alpha=0.05):\n",
    "    \"\"\"Detect concept drift using KS-test on feature distributions.\"\"\"\n",
    "    \n",
    "    p_values = [ks_2samp(X_old[:, i], X_new[:, i]).pvalue for i in range(X_old.shape[1])]\n",
    "    return np.any(np.array(p_values) < alpha)\n",
    "\n",
    "def histogram_binning(X, bins=25):\n",
    "    \"\"\"Convert sample distributions into histograms.\"\"\"\n",
    "    \n",
    "    return np.array([np.histogram(X[:, i], bins=bins, density=True)[0] for i in range(X.shape[1])]).T\n",
    "\n",
    "def kl_divergence(P, Q):\n",
    "    \"\"\"Compute KL divergence between two distributions.\"\"\"\n",
    "    \n",
    "    P, Q = np.clip(P, 1e-10, None), np.clip(Q, 1e-10, None)  # Avoid log(0)\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "\n",
    "def strategic_sample_selection(X_old, X_new, top_k=100, learning_rate=0.01, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Selects representative new samples by minimizing KL divergence.\n",
    "    \n",
    "    Args:\n",
    "        X_old (numpy.ndarray): Old memory buffer samples.\n",
    "        X_new (numpy.ndarray): Incoming new samples.\n",
    "        top_k (int): Number of samples to retain.\n",
    "        learning_rate (float): Step size for optimization.\n",
    "        num_iterations (int): Number of optimization steps.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Selected representative new samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    H_old, H_new = histogram_binning(X_old), histogram_binning(X_new)\n",
    "    m_n = np.random.rand(H_new.shape[0])  \n",
    "\n",
    "    def loss_function(m_n):\n",
    "        \"\"\"Computes KL divergence loss for selected samples.\"\"\"\n",
    "        weighted_H_new = H_new * m_n[:, np.newaxis]  \n",
    "        combined_H = (H_old + weighted_H_new) / 2 \n",
    "        return kl_divergence(H_new, combined_H) \n",
    "\n",
    "    progress_bar = tqdm(total=num_iterations, desc=\"Optimizing Sample Selection\", position=0, leave=True)\n",
    "\n",
    "    def callback(xk):\n",
    "        progress_bar.update(1)  \n",
    "\n",
    "    result = minimize(loss_function, m_n, method=\"L-BFGS-B\", bounds=[(0, 1)] * len(m_n), \n",
    "                      options={\"maxiter\": num_iterations, \"ftol\": 1e-10}, callback=callback)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    selected_indices = np.argsort(result.x)[-top_k:]\n",
    "\n",
    "    return X_new[selected_indices] \n",
    "\n",
    "\n",
    "def update_memory_buffer(X_old, X_new_selected, memory_size=3000):\n",
    "    \"\"\"Updates memory buffer using strategic forgetting.\"\"\"\n",
    "    updated_buffer = np.vstack((X_old, X_new_selected))  \n",
    "\n",
    "    if updated_buffer.shape[0] > memory_size:\n",
    "        updated_buffer = updated_buffer[-memory_size:]\n",
    "\n",
    "    return updated_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf727f9-dbc5-4216-8a1b-4fadc5419337",
   "metadata": {},
   "source": [
    "## Scenario Design + Evaluation Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d267b4db-ae8f-4c28-b0f9-4f9a580a70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_design(normal_data, anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 to create a lifelong learning scenario.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (normal_concept, anomaly_concept) pairs forming the scenario.\n",
    "    \"\"\"\n",
    "    normal_concepts = create_phi(normal_data, c)\n",
    "    anomaly_concepts = create_gamma(anomaly_data, c)\n",
    "    \n",
    "    scenario = match_lambda(anomaly_concepts, normal_concepts)\n",
    "    \n",
    "    return scenario\n",
    "\n",
    "def evaluation_protocol(T, E, Y, model, strategy=\"naive\", replay_buffer_size=3000, memory_size=3000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2: Lifelong Learning Evaluation Protocol with multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        T (list): Sequence of N training sets.\n",
    "        E (list): Sequence of N testing sets.\n",
    "        Y (list): Sequence of true labels for test sets.\n",
    "        model (sklearn.base.BaseEstimator): A scikit-learn-like model instance that supports `fit` and `decision_function`.\n",
    "        strategy (str): Strategy for training.\n",
    "        replay_buffer_size (int): Maximum size of replay buffer if applicable\n",
    "\n",
    "    Returns:\n",
    "        numpy array: NxN results matrix R where R[i, j] is ROC-AUC of model on E[j] after learning T[i].\n",
    "    \"\"\"\n",
    "    N = len(T)\n",
    "    R = np.zeros((N, N))  \n",
    "\n",
    "    if strategy in [\"cumulative\"]:\n",
    "        cumulative_data = []\n",
    "    \n",
    "    if strategy in [\"replay\"]:\n",
    "        replay_buffer = []\n",
    "\n",
    "    if strategy == \"SSF\":\n",
    "        memory_buffer = None \n",
    "\n",
    "    \n",
    "    for i, Ti in tqdm(enumerate(T), desc=f\"Evaluating using {strategy} strategy\"):\n",
    "        current_model = clone(model)\n",
    "\n",
    "        # -- NAIVE --\n",
    "        if strategy == \"naive\":\n",
    "            current_model.fit(Ti)\n",
    "\n",
    "        # -- CUMULATIVE --\n",
    "        if strategy == \"cumulative\":\n",
    "            cumulative_data.extend(Ti.tolist())\n",
    "            current_model.fit(np.array(cumulative_data)) \n",
    "\n",
    "        # -- REPLAY -- \n",
    "        if strategy == \"replay\":\n",
    "            if replay_buffer:\n",
    "                combined_data = np.vstack((np.array(replay_buffer), Ti))\n",
    "            else:\n",
    "                combined_data = Ti\n",
    "\n",
    "            current_model.fit(combined_data)\n",
    "\n",
    "            replay_buffer.extend(Ti.tolist())\n",
    "\n",
    "            if len(replay_buffer) > replay_buffer_size:\n",
    "                replay_buffer = replay_buffer[-replay_buffer_size:]\n",
    "        \n",
    "        # -- SSF -- \n",
    "        if strategy == \"SSF\":\n",
    "            if memory_buffer is None:\n",
    "                memory_buffer = Ti  \n",
    "            else:\n",
    "                drift_detected = kolmogorov_smirnov_test(memory_buffer, Ti, alpha)\n",
    "\n",
    "                if drift_detected:\n",
    "                    X_new_selected = strategic_sample_selection(memory_buffer, Ti, top_k=1000)\n",
    "                    memory_buffer = update_memory_buffer(memory_buffer, X_new_selected, memory_size=memory_size)\n",
    "\n",
    "            current_model.fit(memory_buffer)\n",
    "\n",
    "        # Eval\n",
    "        for j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) in enumerate(zip(E, Y)):\n",
    "            test_data = np.vstack((Ej_normal, Ej_anomaly))\n",
    "            test_labels = np.hstack((y_normal, y_anomaly))  \n",
    "        \n",
    "            scores = -current_model.decision_function(test_data)  \n",
    "            R[i, j] = roc_auc_score(test_labels, scores)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461297b-598d-4ea0-9e43-5ba502595a3e",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20004cfd-6859-4f18-be74-ee20746809d3",
   "metadata": {},
   "source": [
    "### First concatenate all our X_ and y data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c279bf9e-5173-4273-ab10-d9cd687efae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((train_normal_latent_features, train_anomaly_latent_features, val_latent_features, test_latent_features))\n",
    "y = np.hstack((y_train_normal, y_train_anomaly, y_val, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750ca1a-c38b-4396-86e2-de703f2c6636",
   "metadata": {},
   "source": [
    "### Create 'c' normal/anomaly concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0dd6f88e-25a2-4669-a452-7e949e9243fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating normal concepts\n",
      "Finished creating anomaly concepts\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 5\n",
    "\n",
    "X_normal = X[y == 0]  \n",
    "X_anomaly = X[y == 1]\n",
    "\n",
    "normal_concepts = create_phi(X_normal, num_concepts)\n",
    "anomaly_concepts = create_gamma(X_anomaly, num_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1edae2-78bc-4506-8c4b-ba8ac3521a22",
   "metadata": {},
   "source": [
    "### Use lambda function to pair normal/anomaly concepts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "64e97074-165e-46f9-be2c-a315b929dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished matching concept pairs\n"
     ]
    }
   ],
   "source": [
    "concept_pairs = match_lambda(anomaly_concepts, normal_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187e215-35f5-458e-a743-7805800d97fe",
   "metadata": {},
   "source": [
    "### Creating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "16d85d56-1f4e-428e-8b25-3ccf46b205f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []  \n",
    "E = [] \n",
    "Y = []\n",
    "\n",
    "for normal, anomaly in concept_pairs:\n",
    "\n",
    "    normal_train, normal_test = train_test_split(normal, test_size=0.3, random_state=42)\n",
    "    anomaly_train, anomaly_test = train_test_split(anomaly, test_size=0.3, random_state=42)  \n",
    "\n",
    "    T.append(normal_train)\n",
    "    E.append((normal_test, anomaly_test))\n",
    "\n",
    "    y_normal_test = np.zeros(len(normal_test))\n",
    "    y_anomaly_test = np.ones(len(anomaly_test))\n",
    "    \n",
    "    Y.append((y_normal_test, y_anomaly_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb3bab-02cb-455b-ab1b-f5f0a44dd920",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4063dd-99f8-4046-bad5-bd70653d3ca3",
   "metadata": {},
   "source": [
    "## LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "070dd790-8620-4479-8f98-21899c3d55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating using SSF strategy: 0it [00:00, ?it/s]\u001b[A\n",
      "Optimizing Sample Selection:   3%|▎         | 3/100 [00:00<00:00, 305.77it/s]\n",
      "\n",
      "Optimizing Sample Selection:   5%|▌         | 5/100 [00:00<00:00, 387.28it/s]\n",
      "\n",
      "Optimizing Sample Selection:   3%|▎         | 3/100 [00:00<00:00, 325.25it/s]\n",
      "\n",
      "Optimizing Sample Selection:   2%|▏         | 2/100 [00:00<00:00, 249.79it/s]\n",
      "\n",
      "Evaluating using SSF strategy: 5it [00:22,  4.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.8569586758394906, BWT: -0.02945648319218145, FWT: 0.39420217198450735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "482f294f-f5b7-42db-808d-e2848f5d7193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:22,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.7269874370592383, BWT: -0.35246379860486754, FWT: 0.5636736891377074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f69e7c8-cdef-43a7-9220-1e78b59c9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [43:00, 516.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.9581242476119974, BWT: -0.001626170802451321, FWT: 0.3184781803877492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4044123-b410-4198-b4a2-75604b283c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [12:13, 146.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.8231447825592024, BWT: -0.2066602380719666, FWT: 0.43956059766470873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed817146-2f60-4c61-9e62-c97fc39ffde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
