{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421755df-c4c3-4adb-b0c1-74c5910777b9",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e609f474-dd80-40fd-b456-a953f6253fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from copy import deepcopy\n",
    "from pyDeepInsight import ImageTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.linear_model import SGDOneClassSVM\n",
    "from sklearn.base import clone\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import ks_2samp\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fad6262-cb13-420b-9972-45fd7445347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49e23bcc-3c21-48cb-aa68-49dcd56a5fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('dataset/cic-ids-2017/X_train.npy')\n",
    "y_train = np.load('dataset/cic-ids-2017/y_train.npy')\n",
    "\n",
    "X_val = np.load('dataset/cic-ids-2017/X_val.npy')\n",
    "y_val = np.load('dataset/cic-ids-2017/y_val.npy')\n",
    "\n",
    "X_test = np.load('dataset/cic-ids-2017/X_test.npy')\n",
    "y_test = np.load('dataset/cic-ids-2017/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91624f8-9ebf-4a52-b61e-8988c0834f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == 0, 0, 1)\n",
    "y_test = np.where(y_test == 0, 0, 1)\n",
    "y_val = np.where(y_val == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1538386-1a40-44be-aaed-47aa12856b72",
   "metadata": {},
   "source": [
    "# Concept Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488a0f3-04fa-47ed-b2dc-2bb4ea2f8211",
   "metadata": {},
   "source": [
    "- SAFE has 4 modules. We are assuming that the MAE is pre-trained already and we have already used the encoder head to extract latent features. We are implementing this on the LOF in module 4 (novelty detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b5b75-554e-4024-98a9-9c96e42f455b",
   "metadata": {},
   "source": [
    "## Creation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f992c3-e4de-49be-bfac-47bdce14e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi(normal_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for normal data.\n",
    "    Uses k-Means clustering to partition normal data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        c (int): Number of desired normal concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of normal clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(normal_data)\n",
    "    \n",
    "    normal_concepts = [normal_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating normal concepts\")\n",
    "    \n",
    "    return normal_concepts\n",
    "\n",
    "\n",
    "def create_gamma(anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Concept creation function for anomaly data.\n",
    "    Uses k-Means clustering to partition anomaly data into c clusters.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired anomaly concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of numpy arrays: List of anomaly clusters.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=c, random_state=42)\n",
    "    labels = kmeans.fit_predict(anomaly_data)\n",
    "    \n",
    "    anomaly_concepts = [anomaly_data[labels == i] for i in range(c)]\n",
    "    print(\"Finished creating anomaly concepts\")\n",
    "    \n",
    "    return anomaly_concepts\n",
    "    \n",
    "def match_lambda(anomaly_concepts, normal_concepts):\n",
    "    \"\"\"\n",
    "    Matches each normal concept with the closest anomaly concept.\n",
    "    Uses Euclidean distance to determine the best match.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_concepts (list of numpy arrays): List of anomaly clusters.\n",
    "        normal_concepts (list of numpy arrays): List of normal clusters.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Pairs of (normal_concept, matched_anomaly_concept)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    remaining_anomalies = anomaly_concepts.copy()\n",
    "\n",
    "    for normal_concept in normal_concepts:\n",
    "        normal_centroid = np.mean(normal_concept, axis=0)\n",
    "        anomaly_centroids = [np.mean(ac, axis=0) for ac in remaining_anomalies]\n",
    "\n",
    "        distances = cdist([normal_centroid], anomaly_centroids, metric='euclidean')[0]\n",
    "        closest_idx = np.argmin(distances)\n",
    "\n",
    "        pairs.append((normal_concept, remaining_anomalies[closest_idx]))\n",
    "        remaining_anomalies.pop(closest_idx)\n",
    "\n",
    "    print(\"Finished matching concept pairs\")\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d08c06-732c-4eac-9e63-cfcd3691290a",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f9bdeb1-c8e5-41da-a84b-f166c1cc130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lifelong_roc_auc(R):\n",
    "    \"\"\"\n",
    "    Computes the Lifelong ROC-AUC metric.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN matrix of ROC-AUC scores, where R[i, j] is the model's \n",
    "                         performance on concept j after learning concept i.\n",
    "    \n",
    "    Returns:\n",
    "        float: Lifelong ROC-AUC score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    lower_triangular_sum = np.sum(np.tril(R))\n",
    "    normalization_factor = (N * (N + 1)) / 2\n",
    "\n",
    "    return lower_triangular_sum / normalization_factor\n",
    "\n",
    "def BWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Backward Transfer (BWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: BWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    backward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(1, N):\n",
    "        for j in range(i):\n",
    "            backward_transfer += (R[i, j] - R[j, j])\n",
    "            count += 1\n",
    "\n",
    "    return backward_transfer / count if count > 0 else 0\n",
    "\n",
    "def FWT(R):\n",
    "    \"\"\"\n",
    "    Computes the Forward Transfer (FWT) score.\n",
    "    \n",
    "    Args:\n",
    "        R (numpy array): NxN results matrix.\n",
    "    \n",
    "    Returns:\n",
    "        float: FWT score.\n",
    "    \"\"\"\n",
    "    N = R.shape[0]\n",
    "    forward_transfer = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N): \n",
    "            forward_transfer += R[i, j]\n",
    "            count += 1\n",
    "\n",
    "    return forward_transfer / count if count > 0 else 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669576fe-cbe8-4f1e-918b-e2d4ed5bfadb",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71703029-8cad-4317-be17-d74f2a4a252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kolmogorov_smirnov_test(X_old, X_new, alpha=0.05):\n",
    "    \"\"\"Detect concept drift using KS-test on feature distributions.\"\"\"\n",
    "    \n",
    "    p_values = [ks_2samp(X_old[:, i], X_new[:, i]).pvalue for i in range(X_old.shape[1])]\n",
    "    return np.any(np.array(p_values) < alpha)\n",
    "\n",
    "def histogram_binning(X, bins=25):\n",
    "    \"\"\"Convert sample distributions into histograms.\"\"\"\n",
    "    \n",
    "    return np.array([np.histogram(X[:, i], bins=bins, density=True)[0] for i in range(X.shape[1])]).T\n",
    "\n",
    "def kl_divergence(P, Q):\n",
    "    \"\"\"Compute KL divergence between two distributions.\"\"\"\n",
    "    \n",
    "    P, Q = np.clip(P, 1e-10, None), np.clip(Q, 1e-10, None)  # Avoid log(0)\n",
    "    return np.sum(P * np.log(P / Q))\n",
    "\n",
    "def strategic_sample_selection(X_old, X_new, top_k=100, learning_rate=0.01, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Selects representative new samples by minimizing KL divergence.\n",
    "    \n",
    "    Args:\n",
    "        X_old (numpy.ndarray): Old memory buffer samples.\n",
    "        X_new (numpy.ndarray): Incoming new samples.\n",
    "        top_k (int): Number of samples to retain.\n",
    "        learning_rate (float): Step size for optimization.\n",
    "        num_iterations (int): Number of optimization steps.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Selected representative new samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    H_old, H_new = histogram_binning(X_old), histogram_binning(X_new)\n",
    "    m_n = np.random.rand(H_new.shape[0])  \n",
    "\n",
    "    def loss_function(m_n):\n",
    "        \"\"\"Computes KL divergence loss for selected samples.\"\"\"\n",
    "        weighted_H_new = H_new * m_n[:, np.newaxis]  \n",
    "        combined_H = (H_old + weighted_H_new) / 2 \n",
    "        return kl_divergence(H_new, combined_H) \n",
    "\n",
    "    progress_bar = tqdm(total=num_iterations, desc=\"Optimizing Sample Selection\", position=0, leave=True)\n",
    "\n",
    "    def callback(xk):\n",
    "        progress_bar.update(1)  \n",
    "\n",
    "    result = minimize(loss_function, m_n, method=\"L-BFGS-B\", bounds=[(0, 1)] * len(m_n), \n",
    "                      options={\"maxiter\": num_iterations, \"ftol\": 1e-10}, callback=callback)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    selected_indices = np.argsort(result.x)[-top_k:]\n",
    "\n",
    "    return X_new[selected_indices] \n",
    "\n",
    "\n",
    "def update_memory_buffer(X_old, X_new_selected, memory_size=3000):\n",
    "    \"\"\"Updates memory buffer using strategic forgetting.\"\"\"\n",
    "    updated_buffer = np.vstack((X_old, X_new_selected))  \n",
    "\n",
    "    if updated_buffer.shape[0] > memory_size:\n",
    "        updated_buffer = updated_buffer[-memory_size:]\n",
    "\n",
    "    return updated_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf727f9-dbc5-4216-8a1b-4fadc5419337",
   "metadata": {},
   "source": [
    "## Scenario Design + Evaluation Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d267b4db-ae8f-4c28-b0f9-4f9a580a70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_design(normal_data, anomaly_data, c):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 1 to create a lifelong learning scenario.\n",
    "    \n",
    "    Args:\n",
    "        normal_data (numpy array): The normal data points.\n",
    "        anomaly_data (numpy array): The anomaly data points.\n",
    "        c (int): Number of desired concepts.\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: List of (normal_concept, anomaly_concept) pairs forming the scenario.\n",
    "    \"\"\"\n",
    "    normal_concepts = create_phi(normal_data, c)\n",
    "    anomaly_concepts = create_gamma(anomaly_data, c)\n",
    "    \n",
    "    scenario = match_lambda(anomaly_concepts, normal_concepts)\n",
    "    \n",
    "    return scenario\n",
    "\n",
    "def evaluation_protocol(T, E, Y, model, strategy=\"naive\", replay_buffer_size=3000, memory_size=3000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Implements Algorithm 2: Lifelong Learning Evaluation Protocol with multiple strategies.\n",
    "    \n",
    "    Args:\n",
    "        T (list): Sequence of N training sets.\n",
    "        E (list): Sequence of N testing sets.\n",
    "        Y (list): Sequence of true labels for test sets.\n",
    "        model (sklearn.base.BaseEstimator): A scikit-learn-like model instance that supports `fit` and `decision_function`.\n",
    "        strategy (str): Strategy for training.\n",
    "        replay_buffer_size (int): Maximum size of replay buffer if applicable\n",
    "\n",
    "    Returns:\n",
    "        numpy array: NxN results matrix R where R[i, j] is ROC-AUC of model on E[j] after learning T[i].\n",
    "    \"\"\"\n",
    "    N = len(T)\n",
    "    R = np.zeros((N, N))  \n",
    "\n",
    "    if strategy in [\"cumulative\"]:\n",
    "        cumulative_data = []\n",
    "    \n",
    "    if strategy in [\"replay\"]:\n",
    "        replay_buffer = []\n",
    "\n",
    "    if strategy == \"SSF\":\n",
    "        memory_buffer = None \n",
    "\n",
    "    \n",
    "    for i, Ti in tqdm(enumerate(T), desc=f\"Evaluating using {strategy} strategy\"):\n",
    "        current_model = clone(model)\n",
    "\n",
    "        # -- NAIVE --\n",
    "        if strategy == \"naive\":\n",
    "            current_model.fit(Ti)\n",
    "\n",
    "        # -- CUMULATIVE --\n",
    "        if strategy == \"cumulative\":\n",
    "            cumulative_data.extend(Ti.tolist())\n",
    "            current_model.fit(np.array(cumulative_data)) \n",
    "\n",
    "        # -- REPLAY -- \n",
    "        if strategy == \"replay\":\n",
    "            if replay_buffer:\n",
    "                combined_data = np.vstack((np.array(replay_buffer), Ti))\n",
    "            else:\n",
    "                combined_data = Ti\n",
    "\n",
    "            current_model.fit(combined_data)\n",
    "\n",
    "            replay_buffer.extend(Ti.tolist())\n",
    "\n",
    "            if len(replay_buffer) > replay_buffer_size:\n",
    "                replay_buffer = replay_buffer[-replay_buffer_size:]\n",
    "        \n",
    "        # -- SSF -- \n",
    "        if strategy == \"SSF\":\n",
    "            if memory_buffer is None:\n",
    "                memory_buffer = Ti  \n",
    "            else:\n",
    "                drift_detected = kolmogorov_smirnov_test(memory_buffer, Ti, alpha)\n",
    "\n",
    "                if drift_detected:\n",
    "                    X_new_selected = strategic_sample_selection(memory_buffer, Ti, top_k=1000)\n",
    "                    memory_buffer = update_memory_buffer(memory_buffer, X_new_selected, memory_size=memory_size)\n",
    "\n",
    "            current_model.fit(memory_buffer)\n",
    "\n",
    "        # Eval\n",
    "        for j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) in enumerate(zip(E, Y)):\n",
    "            test_data = np.vstack((Ej_normal, Ej_anomaly))\n",
    "            test_labels = np.hstack((y_normal, y_anomaly))  \n",
    "        \n",
    "            scores = -current_model.decision_function(test_data)  \n",
    "            R[i, j] = roc_auc_score(test_labels, scores)\n",
    "            \n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461297b-598d-4ea0-9e43-5ba502595a3e",
   "metadata": {},
   "source": [
    "## Generating Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20004cfd-6859-4f18-be74-ee20746809d3",
   "metadata": {},
   "source": [
    "### First concatenate all our X_ and y data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c279bf9e-5173-4273-ab10-d9cd687efae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((X_train, X_val, X_test))\n",
    "y = np.hstack((y_train.reshape(-1), y_val.reshape(-1), y_test.reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750ca1a-c38b-4396-86e2-de703f2c6636",
   "metadata": {},
   "source": [
    "### Create 'c' normal/anomaly concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dd6f88e-25a2-4669-a452-7e949e9243fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished creating normal concepts\n",
      "Finished creating anomaly concepts\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 5\n",
    "\n",
    "X_normal = X[y == 0]  \n",
    "X_anomaly = X[y == 1]\n",
    "\n",
    "normal_concepts = create_phi(X_normal, num_concepts)\n",
    "anomaly_concepts = create_gamma(X_anomaly, num_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1edae2-78bc-4506-8c4b-ba8ac3521a22",
   "metadata": {},
   "source": [
    "### Use lambda function to pair normal/anomaly concepts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e97074-165e-46f9-be2c-a315b929dc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished matching concept pairs\n"
     ]
    }
   ],
   "source": [
    "concept_pairs = match_lambda(anomaly_concepts, normal_concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187e215-35f5-458e-a743-7805800d97fe",
   "metadata": {},
   "source": [
    "### Creating training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d85d56-1f4e-428e-8b25-3ccf46b205f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = []  \n",
    "E = [] \n",
    "Y = []\n",
    "\n",
    "for normal, anomaly in concept_pairs:\n",
    "\n",
    "    normal_train, normal_test = train_test_split(normal, test_size=0.3, random_state=42)\n",
    "    anomaly_train, anomaly_test = train_test_split(anomaly, test_size=0.3, random_state=42)  \n",
    "\n",
    "    T.append(normal_train)\n",
    "    E.append((normal_test, anomaly_test))\n",
    "\n",
    "    y_normal_test = np.zeros(len(normal_test))\n",
    "    y_anomaly_test = np.ones(len(anomaly_test))\n",
    "    \n",
    "    Y.append((y_normal_test, y_anomaly_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff932189-930f-4d35-8702-e80d45bc1981",
   "metadata": {},
   "source": [
    "# SAFE Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284b677-4c43-4683-a7f1-aa66558bafca",
   "metadata": {},
   "source": [
    "## PCA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52457411-d0c2-40dc-9cf0-2c9577c2c86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_feature_selection(X, k, explained_variance_threshold=0.95):\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumulative_variance >= explained_variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X)\n",
    "\n",
    "    feature_importance = np.abs(pca.components_).sum(axis=0)\n",
    "    top_k_indices = np.argsort(feature_importance)[-k:]\n",
    "    \n",
    "    return top_k_indices\n",
    "\n",
    "k = 31\n",
    "top_features_indices = pca_feature_selection(X_train, k)\n",
    "X_train = X_train[:, top_features_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da4b62-4d45-45f9-8e5f-811b0a72beaa",
   "metadata": {},
   "source": [
    "## PyDeepInsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d68ffeb-726c-4083-a717-80d2f9c4ef97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyDeepInsight.image_transformer.ImageTransformer at 0x7f1972554c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = ImageTransformer(\n",
    "    pixels=8,\n",
    "    feature_extractor='tsne',\n",
    "    discretization='lsa'\n",
    ")\n",
    "it.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4288166-7929-4afd-bc24-c2ecf3659c72",
   "metadata": {},
   "source": [
    "## MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08acf3e3-f1e1-4a8b-8e87-49aae4b2a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(img_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(32 * 2 * 2, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))  # Output: (batch_size, 16, 8, 8)\n",
    "        x = self.pool(x)              # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.relu(self.conv2(x))  # Output: (batch_size, 32, 4, 4)\n",
    "        x = self.pool(x)              # Output: (batch_size, 32, 2, 2)\n",
    "        x = x.view(x.size(0), -1)     # Flatten to (batch_size, 128)\n",
    "        x = self.fc1(x)               # Output: (batch_size, feature_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149b7000-9c0d-4e55-9224-532d6d307ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc2 = nn.Linear(feature_dim, 32 * 2 * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16, img_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.relu(self.fc2(z))           # Output: (batch_size, 128)\n",
    "        x = x.view(x.size(0), 32, 2, 2)      # Reshape to (batch_size, 32, 2, 2)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 32, 4, 4)\n",
    "        x = self.relu(self.deconv1(x))       # Output: (batch_size, 16, 4, 4)\n",
    "        x = self.upsample(x)                 # Upsample to (batch_size, 16, 8, 8)\n",
    "        x = self.sigmoid(self.deconv2(x))    # Output: (batch_size, img_channels, 8, 8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3ddbeb3-0fe5-4862-bd86-128ee846cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(nn.Module):\n",
    "    def __init__(self, img_channels=1, feature_dim=32, latent_dim=2):\n",
    "        super(MAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, feature_dim, latent_dim)\n",
    "        self.decoder = Decoder(img_channels, feature_dim)\n",
    "\n",
    "    def mask_input(self, x, mask_ratio=0.25):\n",
    "        # Generate a mask with 0s and 1s, keeping only (1-mask_ratio) of the original input\n",
    "        mask = torch.rand(x.shape, device=x.device) > mask_ratio\n",
    "        x_masked = x * mask\n",
    "        return x_masked, mask\n",
    "\n",
    "    def forward(self, x, mask_ratio=0.25):\n",
    "        x_masked, mask = self.mask_input(x, mask_ratio)  # Apply masking to input\n",
    "        z = self.encoder(x_masked)\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3553c18-5547-4922-9e60-ee812261ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss_function(reconstructed, original, mask):\n",
    "    # Only calculate reconstruction loss on the masked parts\n",
    "    masked_original = original * mask\n",
    "    reconstruction_loss = F.mse_loss(reconstructed, masked_original, reduction='sum')\n",
    "    return reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b4a812-cc94-4fc1-8502-d3ca53c82a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latent_features(model, data_loader, device='cuda'):\n",
    "    model.eval() \n",
    "    latent_features = []  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, total=len(data_loader), desc=\"Extracting features\"):\n",
    "            if len(batch) == 2:\n",
    "                data, _ = batch  \n",
    "            else:\n",
    "                (data,) = batch  \n",
    "            \n",
    "            data = data.to(device)\n",
    "\n",
    "            latent_feature = model.encoder(data)\n",
    "            latent_features.append(latent_feature.cpu().numpy())\n",
    "\n",
    "    latent_features = np.concatenate(latent_features, axis=0)\n",
    "    \n",
    "    return latent_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "714af6af-8bc3-4a05-ac7b-ccc46ace436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_process(X, type, y=None):\n",
    "    X_selected = X[:, top_features_indices]\n",
    "    X_images = it.transform(X_selected, 'pytorch')\n",
    "\n",
    "    model = MAE(img_channels=3, feature_dim=32, latent_dim=16).to(device)\n",
    "    model.load_state_dict(torch.load(\"mae/cic-ids-2017/mae_3.10.pth\", weights_only=True))\n",
    "\n",
    "    batch_size = 32\n",
    "    if type == 'T':\n",
    "        tensor_x = X_images.clone().detach().float()\n",
    "        dataset = TensorDataset(tensor_x)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    else: # type == 'E'\n",
    "        tensor_x = X_images.clone().detach().float()\n",
    "        tensor_y = torch.tensor(y, dtype=torch.long)\n",
    "        dataset = TensorDataset(tensor_x, tensor_y)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "    return extract_latent_features(model, loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3efcf97d-9907-4068-add2-bfff041a62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 65875/65875 [00:32<00:00, 2019.19it/s]\n",
      "Extracting features: 100%|██████████| 16589/16589 [00:07<00:00, 2161.06it/s]\n",
      "Extracting features: 100%|██████████| 27972/27972 [00:13<00:00, 2115.35it/s]\n",
      "Extracting features: 100%|██████████| 24812/24812 [00:11<00:00, 2109.10it/s]\n",
      "Extracting features: 100%|██████████| 13927/13927 [00:07<00:00, 1937.09it/s]\n",
      "Extracting features: 100%|██████████| 28232/28232 [00:15<00:00, 1816.08it/s]\n",
      "Extracting features: 100%|██████████| 5239/5239 [00:02<00:00, 1914.65it/s]\n",
      "Extracting features: 100%|██████████| 7110/7110 [00:03<00:00, 1839.54it/s]\n",
      "Extracting features: 100%|██████████| 3588/3588 [00:01<00:00, 1853.88it/s]\n",
      "Extracting features: 100%|██████████| 11988/11988 [00:06<00:00, 1794.27it/s]\n",
      "Extracting features: 100%|██████████| 2195/2195 [00:01<00:00, 1787.98it/s]\n",
      "Extracting features: 100%|██████████| 10634/10634 [00:05<00:00, 1901.33it/s]\n",
      "Extracting features: 100%|██████████| 3197/3197 [00:01<00:00, 1856.03it/s]\n",
      "Extracting features: 100%|██████████| 5969/5969 [00:03<00:00, 1767.47it/s]\n",
      "Extracting features: 100%|██████████| 1467/1467 [00:00<00:00, 1807.09it/s]\n"
     ]
    }
   ],
   "source": [
    "T_temp = []\n",
    "E_temp = []\n",
    "\n",
    "for i, Ti in enumerate(T):\n",
    "    T_temp.append(safe_process(Ti, 'T'))\n",
    "\n",
    "for i, Ei in enumerate(E):\n",
    "    E_temp.append((safe_process(Ei[0], 'E', Y[i][0]), safe_process(Ei[1], 'E', Y[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb3bab-02cb-455b-ab1b-f5f0a44dd920",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4063dd-99f8-4046-bad5-bd70653d3ca3",
   "metadata": {},
   "source": [
    "## SGDOCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8daf596-6640-43ad-a248-7057cc248464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:15,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.4067202900251073, BWT: -0.19373292914145945, FWT: 0.36610451926600607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db6ff1f3-7a98-46e2-9236-6fbe929ef86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 238.69it/s]\n",
      "Optimizing Sample Selection:   3%|▎         | 3/100 [00:00<00:00, 148.92it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 285.72it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 275.20it/s]\n",
      "Evaluating using SSF strategy: 5it [00:48,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.29568848762345645, BWT: -0.0380056000184909, FWT: 0.059826549505803295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39da922b-274a-4e97-9a40-0e6c6992788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:36,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.4030846487074668, BWT: -0.1993625832862301, FWT: 0.3650357257317194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d14ebd7-3384-4537-8ae2-c35273256df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [01:59, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.2473934826999464, BWT: -0.13202723569506467, FWT: 0.06982659523084224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, SGDOneClassSVM(), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1729c2-57d2-4d87-bcd7-06bb884a7b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80674042-cae4-49bc-93fd-ac0be59a3ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using naive strategy: 5it [00:12,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.557507006152225, BWT: -0.42087351392855077, FWT: 0.46855227253242315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_naive = evaluation_protocol(T_temp, E_temp, Y, SGDOneClassSVM(), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a63d5cf8-3c9a-45f9-b58b-db00438165f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Sample Selection:   2%|▏         | 2/100 [00:00<00:00, 254.73it/s]\n",
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 297.00it/s]\n",
      "Optimizing Sample Selection:   4%|▍         | 4/100 [00:00<00:00, 393.35it/s]\n",
      "Optimizing Sample Selection:   3%|▎         | 3/100 [00:00<00:00, 353.51it/s]\n",
      "Evaluating using SSF strategy: 5it [00:34,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5454036307352762, BWT: -0.015737101449354522, FWT: 0.6238401123785885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_ssf = evaluation_protocol(T_temp, E_temp, Y, SGDOneClassSVM(), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70f809fd-0a8d-45e2-9095-517776db64a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using replay strategy: 5it [00:22,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.5695162763700956, BWT: -0.3957509114311049, FWT: 0.47026517852041144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "R_replay = evaluation_protocol(T_temp, E_temp, Y, SGDOneClassSVM(), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ade58a3f-2144-4a04-ad04-de2405857068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using cumulative strategy: 5it [01:13, 14.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lifelong ROC-AUC: 0.4811298179631344, BWT: -0.4007376241282838, FWT: 0.620707858226669\n"
     ]
    }
   ],
   "source": [
    "R_cumulative = evaluation_protocol(T_temp, E_temp, Y, SGDOneClassSVM(), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb62cfd-5fd6-4e8f-b6ad-6a1aea416ffe",
   "metadata": {},
   "source": [
    "## LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070dd790-8620-4479-8f98-21899c3d55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating using SSF strategy: 0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "R_ssf = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f294f-f5b7-42db-808d-e2848f5d7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_naive = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f69e7c8-cdef-43a7-9220-1e78b59c9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_cumulative = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4044123-b410-4198-b4a2-75604b283c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_replay = evaluation_protocol(T, E, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed817146-2f60-4c61-9e62-c97fc39ffde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a6a2fc4-5ad8-4f00-8f41-30b0e774cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing Sample Selection:   1%|          | 1/100 [00:00<00:00, 257.73it/s]\n",
      "Evaluating using SSF strategy: 1it [1:04:06, 3846.99s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4617939508042457317 is out of bounds for axis 0 with size 5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m R_ssf \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation_protocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_temp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLocalOutlierFactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnovelty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSSF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLifelong ROC-AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlifelong_roc_auc(R_ssf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, BWT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBWT(R_ssf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, FWT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFWT(R_ssf)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36mevaluation_protocol\u001b[0;34m(T, E, Y, model, strategy, replay_buffer_size, memory_size, alpha)\u001b[0m\n\u001b[1;32m     82\u001b[0m             X_new_selected \u001b[38;5;241m=\u001b[39m strategic_sample_selection(memory_buffer, Ti, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m     83\u001b[0m             memory_buffer \u001b[38;5;241m=\u001b[39m update_memory_buffer(memory_buffer, X_new_selected, memory_size\u001b[38;5;241m=\u001b[39mmemory_size)\n\u001b[0;32m---> 85\u001b[0m     \u001b[43mcurrent_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Eval\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, ((Ej_normal, Ej_anomaly), (y_normal, y_anomaly)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(E, Y)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/neighbors/_lof.py:300\u001b[0m, in \u001b[0;36mLocalOutlierFactor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distances_fit_X_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distances_fit_X_\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    297\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lrd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_local_reachability_density\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distances_fit_X_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_neighbors_indices_fit_X_\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# Compute lof score over training samples to define offset_:\u001b[39;00m\n\u001b[1;32m    305\u001b[0m lrd_ratios_array \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lrd[_neighbors_indices_fit_X_] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lrd[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m    307\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/sklearn/neighbors/_lof.py:514\u001b[0m, in \u001b[0;36mLocalOutlierFactor._local_reachability_density\u001b[0;34m(self, distances_X, neighbors_indices)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_local_reachability_density\u001b[39m(\u001b[38;5;28mself\u001b[39m, distances_X, neighbors_indices):\n\u001b[1;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The local reachability density (LRD)\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    The LRD of a sample is the inverse of the average reachability\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m        The local reachability density of each sample.\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 514\u001b[0m     dist_k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distances_fit_X_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mneighbors_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_neighbors_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    515\u001b[0m     reach_dist_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(distances_X, dist_k)\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# 1e-10 to avoid `nan' when nb of duplicates > n_neighbors_:\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4617939508042457317 is out of bounds for axis 0 with size 5000"
     ]
    }
   ],
   "source": [
    "# Using SAFE\n",
    "R_ssf = evaluation_protocol(T_temp, E_temp, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"SSF\", memory_size=5000, alpha=0.05)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_ssf)}, BWT: {BWT(R_ssf)}, FWT: {FWT(R_ssf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaac024-05c1-40a6-b160-c8a8325fbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_naive = evaluation_protocol(T_temp, E_temp, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"naive\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_naive)}, BWT: {BWT(R_naive)}, FWT: {FWT(R_naive)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a0417-303f-4c05-9517-4f8c59670c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_cumulative = evaluation_protocol(T_temp, E_temp, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"cumulative\")\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_cumulative)}, BWT: {BWT(R_cumulative)}, FWT: {FWT(R_cumulative)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375fe79-c460-4f3d-983d-0e1356c73d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_replay = evaluation_protocol(T_temp, E_temp, Y, LocalOutlierFactor(n_neighbors=20, novelty=True), strategy=\"replay\", replay_buffer_size=5000)\n",
    "print(f\"Lifelong ROC-AUC: {lifelong_roc_auc(R_replay)}, BWT: {BWT(R_replay)}, FWT: {FWT(R_replay)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb417b-0aea-41e5-82ec-a979b20b526a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
